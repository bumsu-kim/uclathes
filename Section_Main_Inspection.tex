
\section{Main Results}
One of the core ideas in IR is, at each iteration, to restrict the cost of inspection up to same order of magnitude as $\mathcal{A}$.
This ensures that if both $\mathcal{A}$ and $\mathcal{A}$ with IR are stuck at the same local minimum, the overall sample complexity differs only by a constant factor.
Moreover, this restriction simplifies the complexity analysis imposed by the inspection as it mainly focuses on the iteration counts.
For line-search type methods, like Nesterov-Spokoiny (NS) \cite{nesterov2017random}, Stochastic Three Point (STP) and its momentum variant (SMTP) \cite{bergou2020stochastic,gorbunov2019stochastic}, and CARS from the previous chapter, the number of queries per iteration is $\mathcal{O}(1)$.
On the other hand, for local methods requiring more queries per iteration, such as SPSA \cite{spall1992multivariate}, 2SPSA \cite{spall2000adaptive}, DGS \cite{zhang2020novel}, AdaDGS \cite{tran2020adadgs} and others using an adjustable size of batch of sample directions \cite{mania2018simple,salimans2017evolution},  more inspections can be performed each iteration.


\begin{algorithm}[H]
    \caption{Inspect as You Run}
     \label{alg:IR general}
    \begin{algorithmic}[1]
      \State \textbf{Input:} $x_0$: initial point; $r$: sampling radius; $R$: inspection radius; $\mathcal{A}$: One step of a DFO method, generating the next iterate; $n_k$: maximum number of inspections at $k$-th iteration; $\nu$: descent threshold
      \State Get the oracle $f(x_0)$.
      \For{$k = 1, \cdots, K$}
            \State Compute $x_{k, 0} = \mathcal{A}(x_k)$
            \For {$j = 1, \cdots, n_k$}
                \State Compute $f(x_{k,j})$ 
                \If{$f(x_{k,j}) < f(x_{k,0}) - \nu$}
                    \State Set $x_{k+1} = x_{k,j}$ and \emph{break}
                \EndIf
            \EndFor
            \State If no successful inspections, set $x_{k+1} = x_{k,0}$ \label{eq: inspection step in the algorithm 1}
      \EndFor
       \State \textbf{Output:} $x_K$: estimated optimum point.
    \end{algorithmic}
\end{algorithm}

\subsection{Analysis on the High Probability Guarantee}
This section presents an analysis of the point derived from Algorithm~\ref{alg:IR general}.
The main result states that if the recent iterations are trapped in a small region without successful inspections, then it is an $R_0$-local minimum with high probability, for some $R_0 < R$.

\begin{theorem} \label{thm: high prob guarantee of an approx. R-local min}
    Let $\{x_k\}_{k=1}^{K}$ be the sequence of points obtained from Algorithm~\ref{alg:IR general}. Assume that no successful inspections occur for all $k \geq k_0$, and let $m$ be the total number of inspections for $k \geq k_0$. Suppose
    \begin{align*}
        \|x_{k} - x_K\| \leq D < R \quad \text{for all } k \geq k_0.
    \end{align*}
    Define $R_0 = R-D$ and choose a positive $\tilde{r} < D$. Then, $x_K$ is an $R_0$-local minimum up to $\eta = \bar{L}\tilde{r} + \nu$ with probability at least $1-\exp(- m (\tilde{r}/R)^d)$.
\end{theorem}
\begin{proof}
    Begin by noting that the ball $B(x_K, R_0 + \tilde{r})$ is a subset of $B(x_{k}, R)$ for all $k \geq k_0$. Hence, for a random variable $z \sim \mathrm{Unif}(B(x_{k}, R))$, the conditioned random variable $z|\mathcal{A}$, where $\mathcal{A}$ is the event $z \in B(x_K, R_0 + \tilde{r})$, follows the uniform distribution over $B(x_K, R_0 + \tilde{r})$.

    At iteration $k= K$, define $S = \{y_i\}_{i=1}^{M}$ as the subset of the recent $m$ inspection points contained within $B(x_K, R_0 + \tilde{r})$. Then $M$ follows the binomial distribution $M \sim \mathrm{Binomial}( m , \phi_1)$ with $\phi_1 = \left(\frac{R_0  + \tilde{r}}{R}\right)^d$.
    
    We now demonstrate that if $S$ is sufficiently dense, $x_K$ becomes an approximate $R_0$-local minimum. Consider
    \begin{align*}
        \tilde{x} := \argmin_{x \in B(x_K, R_0)} f(x).
    \end{align*}
    If there exists $y_i \in S \cap B(\tilde{x},\tilde{r})$, knowing $y_i$ is not a successful inspection, we obtain 
    \begin{align*}
        f(x_K) \leq f(\tilde{x}) + (f(y_i) - f(\tilde{x})) + \nu
        \leq \min_{x \in B(x_K, R_0)}f(x) + \bar{L} \tilde{r} + \nu,
    \end{align*}
    implying that $x_K$ is an $R_0$-local minimum up to $\eta = \bar{L}\tilde{r} + \nu$.
    
    Finally, we need to find the probability bound for $S \cap B(\tilde{x}, \tilde{r})$ not being empty. As each $y_j \sim \mathrm{Unif}(B(x_K, R_0 + \tilde{r}))$ and $B(\tilde{x},\tilde{r}) \subseteq B(x_K, R_0 + \tilde{r})$, we get $\phi_2 := \mathbb{P}[y_j \in B(\tilde{x}, \tilde{r})] = \left(\frac{\tilde{r}}{R_0 + \tilde{r}}\right)^d$. Consequently,
    \begin{align*}
        & \quad~\mathbb{P}[y_j \not\in B(\tilde{x}, \tilde{r}) \text{ for all } j = 1, \cdots, M] \\
        & =\sum_{M' = 0}^{\infty} \left(1 - \phi_2\right)^{M'} \mathbb{P}[M = M'] \\
        & =\mathbb{E}_{M}(\exp{(M\log(1-\phi_2))}) \\
        & \stackrel{(a)}{=} (1-\phi_1 + \phi_1 \exp{(\log(1-\phi_2))})^m \\
        & = (1-\phi_1 \phi_2 )^m \\
        & \leq \exp(-m\phi_1 \phi_2), & \text{($1-x \leq \exp(-x)$)}
    \end{align*}
    where (a) is from the moment generating function of binomial distributions.
    And since $\phi_1 \phi_2 = (\tilde{r}/R)^d$, $S \cap B(\tilde{x}, \tilde{r})$ is nonempty with a probability of at least $1 - \exp(-m (\tilde{r}/R)^d)$.
\end{proof}

A direct implication of Theorem~\ref{thm: high prob guarantee of an approx. R-local min} is that if the iterates get trapped and no further successful inspections are found since then, the solution obtained is an approximate $R_0$-local minimum with probability at least $1-\delta$, given the count of such consecutive inspections exceeds  $\log(\delta^{-1}) \left(\frac{R}{\tilde{r}}\right)^d$.

\subsection{Discussion on IR}
Unlike global DFO methods that constructs a surrogate model in the entire domain, it is easier for local methods to exploit the structure of the problem.
For instance, if most variables are not highly correlated, Coordinate Descent (CD) \cite{wright2015coordinate} or Block Coordinate Descent (BCD) \cite{tseng2001convergence} analogue of DFO methods can be extremely useful. For instance, such variants can be easily obtained by replacing the sampling distribution in CARS.
This room for the choice depending on the problem structure is one of the advantages of our method and really fills the gap between the local and global DFO methods.